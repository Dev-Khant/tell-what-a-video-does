# Video Understanding and Q&A Tool

This project allows you to input a YouTube video link, and it provides a comprehensive understanding of the video's content through audio transcription and image captioning. **LLM** is used to combine audio and video context. Additionally, you can ask questions and it will provide responses according to video content üöÄ

## Features ‚ú®

üëâ **Video Understanding**: The tool utilizes the **Transformer** model for audio transcription, converting spoken words into textual format. It also employs image captioning techniques to extract text from images within the video. **Image embeddings** are also used to compare images and only use images unique for extracting info. Video and Audio are processed **parallelly**.

üëâ **Question & Answer**: Users can ask questions about the video's content. The tool leverages the power of **Chromadb** as a vector database to provide accurate and contextually relevant answers.

## How to Use ‚öôÔ∏è

‚Ä¢ Clone this repository: `git clone https://github.com/Dev-Khant/tell-what-a-video-does.git`

‚Ä¢ Install the required dependencies: `pip install -r requirements.txt`

‚Ä¢ Run the streamlit app: `streamlit run app.py`

‚Ä¢ Provide **YouTube video** with your **OpenAI token**, **Huggingface token**, **SerpAPI token**

## Technical üñ•Ô∏è

‚Ä¢ [Hugging Face](https://huggingface.co/): Utilized to access the OpenAI Whisper model for audio transcription.

‚Ä¢ [SerpApi](https://serpapi.com/): Used it to access Google Lens API for getting image information.

‚Ä¢ [Streamlit](https://streamlit.io/): Used to create the interactive web interface for the project.

‚Ä¢ [Chromadb](https://www.trychroma.com/): The vector database used for storing and retrieving Q&A information.

## Work in Progress üöß

1. Add Weaviate and let the user select their VectorDB.
2. Internet access to chatbot.
3. Option to upload video.
4. Store video explanations so they can be used later.
